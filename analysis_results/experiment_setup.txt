=== EXPERIMENT SETUP AND CONFIGURATION ===

1. HARDWARE CONFIGURATION
=======================

GPU Information:
----------------

GPU Utilization Statistics:
Average GPU Utilization: 0.000000%
Average GPU Memory Utilization: 0.000000%
Peak GPU Memory Usage: 0.000000 GB

CPU Information:
----------------

2. SOFTWARE CONFIGURATION
=======================

Python and Framework Versions:
----------------------------

3. MODEL CONFIGURATION
====================
Model Architecture: ModernBERT-base
Model Type: Transformer-based
Task: Vulnerability Detection
Precision: Mixed (FP16/FP32)

4. DATASET CONFIGURATION
======================
Dataset Name: BigVul
Task Type: Vulnerability Detection
Dataset Type: Code Analysis
Language: Source Code

5. TRAINING CONFIGURATION
======================

6. MEASUREMENT CONFIGURATION
=========================

Energy Measurement:
------------------
Energy Measurement Tool: CodeCarbon
Measurement Frequency: Per Training Run

Performance Metrics:
------------------
Evaluation Metrics: F1 Score, Accuracy
Hardware Metrics: GPU Utilization, Memory Usage
Energy Metrics: Total Energy, CPU Energy, GPU Energy

7. VARIANT INFORMATION
=====================
Total Number of Variants: 30
Baseline Variant: v0_baseline

Variant Types:
-------------
Individual Optimizations:
- v0_baseline
- v10_dataloader_pin_memory
- v11_torch_compile
- v12_attention
- v13_layer_pruning_4_top
- v14_layer_pruning_4_bottom
- v15_layer_pruning_8_top
- v16_layer_pruning_8_bottom
- v17_layer_pruning_12_top
- v18_layer_pruning_12_bottom
- v19_layer_pruning_16_top
- v1_gradient_checkpointing
- v20_layer_pruning_16_bottom
- v21_layer_pruning_20_top
- v22_layer_pruning_20_bottom
- v2_lora_peft
- v3_quantization
- v4_tokenizer
- v5_power_limit_100w
- v6_optimizer
- v7_f16
- v8_sequence_length_trimming
- v9_inference_engine

Combined Optimizations:
- v23_attention_plus_pin_memory_plus_optimizer_plus_gradient_accumulation
- v24_inference_engine_plus_grad_cpting_plus_lora_plus_fp16
- v25_gradient_accumulation_plus_fp16_plus_checkpointing
- v26_pruning_plus_seq_lngth_plus_torch_compile
- v27_torch_compile_plus_fp16
- v28_pruning_plus_torch_compile_plus_fp16
- v29_attention_plus_pin_memory_plus_optimizer
