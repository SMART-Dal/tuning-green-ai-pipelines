torch>=2.0.0
transformers[truststore]>=4.30.0
datasets>=2.12.0
numpy>=1.24.0
tqdm>=4.65.0
pynvml>=11.5.0  # For GPU energy monitoring
codecarbon>=2.2.0  # For CPU/system energy monitoring
pytest>=7.0.0  # For testing
black>=22.0.0  # For code formatting
flake8>=4.0.0  # For linting
psutil>=5.9.0
gputil>=1.4.0
hydra-core>=1.3.0
omegaconf>=2.3.0
scikit-learn>=1.0.0  # For metrics and evaluation
pandas>=1.5.0  # For data handling
matplotlib>=3.5.0  # For plotting
seaborn>=0.12.0  # For statistical visualizations
wandb>=0.15.0  # For experiment tracking
accelerate>=0.20.0  # For distributed training
bitsandbytes>=0.41.0  # For quantization
peft>=0.4.0  # For parameter efficient fine-tuning
sentencepiece>=0.1.99  # For tokenization
protobuf>=3.20.0  # Required by some transformers models 